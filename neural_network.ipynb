{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gejZNMUWexQa",
        "outputId": "87ba66f6-62b9-4cb7-f47f-d577299e9d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyedflib in /usr/local/lib/python3.10/dist-packages (0.1.34)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from pyedflib) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyedflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym_j0TIt7nXP",
        "outputId": "37db2012-db98-4567-ca6b-86f6bd2eb5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1193, 128, 128, 1)\n",
            "(1193, 128, 128, 1)\n"
          ]
        }
      ],
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "from scipy.signal import cwt, morlet\n",
        "import tensorflow as tf\n",
        "\n",
        "def normalize_eeg(eeg_data):\n",
        "    # Z-score normalization\n",
        "    mean = np.mean(eeg_data, axis=1, keepdims=True)\n",
        "    std = np.std(eeg_data, axis=1, keepdims=True)\n",
        "    normalized_data = (eeg_data - mean) / std\n",
        "    return normalized_data\n",
        "\n",
        "def segment_eeg(normalized_data, segment_length, overlap):\n",
        "    segments = []\n",
        "    for i in range(0, len(normalized_data[0]) - segment_length + 1, overlap):\n",
        "      segment = normalized_data[:, i:i+segment_length]\n",
        "      segments.append(segment)\n",
        "    segments = np.array(segments)\n",
        "    return segments\n",
        "\n",
        "def cwt_time_frequency(segmented_data, channel_idx, sample_rate):\n",
        "    cwt_matrices = []\n",
        "    for segment in segmented_data:\n",
        "        # Apply Continuous Wavelet Transform (CWT) using Morlet wavelet\n",
        "        cwt_matrix = cwt(segment[channel_idx], morlet, widths=np.arange(1, 100))\n",
        "        cwt_matrices.append(cwt_matrix)\n",
        "    return cwt_matrices\n",
        "\n",
        "def cwt_time_frequency_plot(edf_file_paths, channel_idx, sample_rate):\n",
        "    cwt_matrices_list = []\n",
        "\n",
        "    for edf_file_path in edf_file_paths:\n",
        "        # Load .edf file\n",
        "        edf_file = pyedflib.EdfReader(edf_file_path)\n",
        "\n",
        "        # EEG channel labels based on the 10-20 EEG montage\n",
        "\n",
        "        \n",
        "        eeg_channels = [\n",
        "            \"Fp1\", \"Fp2\", \"F7\", \"F3\", \"Fz\", \"F4\", \"F8\",\n",
        "            \"T3\", \"C3\", \"Cz\", \"C4\", \"T4\", \"T5\", \"P3\",\n",
        "            \"Pz\", \"P4\", \"T6\", \"O1\", \"O2\"\n",
        "        ]\n",
        "\n",
        "        # Read EEG data for the specified channels\n",
        "        eeg_data = [edf_file.readSignal(edf_file.getSignalLabels().index(channel)) for channel in eeg_channels]\n",
        "        # Normalize EEG data\n",
        "        normalized_data = normalize_eeg(np.array(eeg_data))\n",
        "\n",
        "        # Parameters for segmentation\n",
        "        segment_length = int(4 * sample_rate)  # 4 seconds * sample rate\n",
        "        overlap = int(segment_length * 0.75)\n",
        "\n",
        "        # Segment normalized data\n",
        "        segments = segment_eeg(normalized_data, segment_length, overlap)\n",
        "\n",
        "        # Choose a channel index for CWT visualization\n",
        "        selected_channel_idx = channel_idx\n",
        "\n",
        "        # Perform CWT and create time-frequency representation for segmented data\n",
        "        cwt_matrices = cwt_time_frequency(segments, selected_channel_idx, sample_rate)\n",
        "\n",
        "        cwt_matrices_list.extend(cwt_matrices)\n",
        "\n",
        "        # Close .edf file\n",
        "        edf_file.close()\n",
        "\n",
        "    # Stack the CWT matrices along a new axis to combine them\n",
        "    cwt_m = np.array(cwt_matrices_list)\n",
        "\n",
        "    resized_images = []\n",
        "    for sub_matrix in cwt_m:\n",
        "        # Resize the image to 128x128 using bicubic interpolation\n",
        "        sub_matrix = tf.abs(sub_matrix)\n",
        "        sub_matrix = tf.expand_dims(sub_matrix, axis=2)\n",
        "        resized_images.append(tf.image.resize(sub_matrix, (128, 128), method='bicubic'))\n",
        "\n",
        "    resized_images = np.array(resized_images)\n",
        "\n",
        "    return resized_images\n",
        "\n",
        "# List of EDF file paths to combine\n",
        "edf_file_paths = [\"h01.edf\",\"h02.edf\",\"h03.edf\",\"h04.edf\",\"h05.edf\",\"h06.edf\",\"h07.edf\",\"h08.edf\",\"h09.edf\",\"h10.edf\",\"h11.edf\",\"h12.edf\",\"h13.edf\",\"h14.edf\"]  # Add all file paths\n",
        "edf_file_paths1=[\"s01.edf\",\"s02.edf\",\"s03.edf\",\"s04.edf\",\"s05.edf\",\"s06.edf\",\"s07.edf\",\"s08.edf\",\"s09.edf\",\"s10.edf\",\"s11.edf\",\"s12.edf\",\"s13.edf\",\"s14.edf\"]\n",
        "# Test the function with multiple files\n",
        "# edf_file_paths=[\"h01.edf\",\"h02.edf\"]\n",
        "# edf_file_paths1=[\"s01.edf\",\"s02.edf\"]\n",
        "\n",
        "resized_image = cwt_time_frequency_plot(edf_file_paths, 0, 128)\n",
        "resized_image1 = cwt_time_frequency_plot(edf_file_paths, 0, 128)\n",
        "\n",
        "print(resized_image.shape)\n",
        "print(resized_image1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slbZXwPH1DIr",
        "outputId": "437dcdc7-438a-4bf7-c471-6e783b72d3f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(939, 128, 128, 1)\n",
            "(939,)\n",
            "(254, 128, 128, 1)\n",
            "(254,)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# Create a list of resized images\n",
        "resized_images = []\n",
        "\n",
        "# Generate random numbers for each image\n",
        "random_numbers = [random.random() for i in range(len(resized_images))]\n",
        "\n",
        "# Split the images into training and testing sets\n",
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []\n",
        "\n",
        "for i, random_number in enumerate(random_numbers):\n",
        "    if random_number <= 0.8:\n",
        "        X_train.append(resized_images[i])\n",
        "        Y_train.append(0)\n",
        "    else:\n",
        "        X_test.append(resized_images[i])\n",
        "        Y_test.append(0)\n",
        "\n",
        "random_numbers = [random.random() for i in range(len(resized_image1))]\n",
        "\n",
        "for i, random_number in enumerate(random_numbers):\n",
        "    if random_number <= 0.8:\n",
        "        X_train.append(resized_image1[i])\n",
        "        Y_train.append(1)\n",
        "    else:\n",
        "        X_test.append(resized_image1[i])\n",
        "        Y_test.append(1)\n",
        "\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "X_test = np.array(X_test)\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    j = random.randint(0, len(X_train) - 1)\n",
        "    X_train[i], X_train[j] = X_train[j], X_train[i]\n",
        "    Y_train[i], Y_train[j] = Y_train[j], Y_train[i]\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    j = random.randint(0, len(X_test) - 1)\n",
        "    X_test[i],X_test[j] = X_test[j],X_test[i]\n",
        "    Y_test[i], Y_test[j] = Y_test[j], Y_test[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5qvlenNCtM-",
        "outputId": "ee5bbafa-a7bf-4469-9729-2b5f97ab3da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " separable_conv2d (SeparableCon  (None, 128, 128, 16  57         ['input_1[0][0]']                \n",
            " v2D)                           )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_1 (SeparableC  (None, 128, 128, 16  288        ['separable_conv2d[0][0]']       \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_2 (SeparableC  (None, 128, 128, 16  416        ['separable_conv2d_1[0][0]']     \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 128, 128, 48  0           ['separable_conv2d[0][0]',       \n",
            "                                )                                 'separable_conv2d_1[0][0]',     \n",
            "                                                                  'separable_conv2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " separable_conv2d_3 (SeparableC  (None, 128, 128, 32  2000       ['concatenate[0][0]']            \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_4 (SeparableC  (None, 128, 128, 32  2768       ['concatenate[0][0]']            \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_5 (SeparableC  (None, 128, 128, 32  1616       ['concatenate[0][0]']            \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_6 (SeparableC  (None, 128, 128, 32  1344       ['separable_conv2d_3[0][0]']     \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_7 (SeparableC  (None, 128, 128, 32  1856       ['separable_conv2d_4[0][0]']     \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " separable_conv2d_8 (SeparableC  (None, 128, 128, 32  1088       ['separable_conv2d_5[0][0]']     \n",
            " onv2D)                         )                                                                 \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 128, 128, 96  41568       ['concatenate[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 128, 128, 96  41568       ['concatenate[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 128, 128, 96  0           ['separable_conv2d_6[0][0]',     \n",
            "                                )                                 'separable_conv2d_7[0][0]',     \n",
            "                                                                  'separable_conv2d_8[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 128, 96  384        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 128, 128, 96  384        ['conv2d_1[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 64, 64, 96)   0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 128, 128, 96  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 128, 128, 96  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 96)          0           ['max_pooling2d[0][0]']          \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 128, 128, 1)  865         ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 128, 128, 1)  865         ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 96)           9312        ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 128, 128, 1)  4          ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 128, 128, 1)  4          ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 96)           9312        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 128, 128, 1)  0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 128, 128, 1)  0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 96)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 128, 128, 1)  0           ['activation_3[0][0]',           \n",
            "                                                                  'activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " tf.expand_dims (TFOpLambda)    (None, 1, 96)        0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 128, 128, 1)  0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " tf.expand_dims_1 (TFOpLambda)  (None, 1, 1, 96)     0           ['tf.expand_dims[0][0]']         \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 128, 128, 1)  0           ['input_1[0][0]',                \n",
            "                                                                  'activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 64, 64, 96)   0           ['max_pooling2d[0][0]',          \n",
            "                                                                  'tf.expand_dims_1[0][0]']       \n",
            "                                                                                                  \n",
            " tf.image.resize (TFOpLambda)   (None, 64, 64, 1)    0           ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " tf.image.resize_1 (TFOpLambda)  (None, 64, 64, 96)  0           ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 64, 64, 97)   0           ['tf.image.resize[0][0]',        \n",
            "                                                                  'tf.image.resize_1[0][0]']      \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 97)  0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " global_average_pooling2d_1 (Gl  (None, 97)          0           ['max_pooling2d_1[0][0]']        \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 128)          12544       ['global_average_pooling2d_1[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 64)           8256        ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 2)            130         ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 136,629\n",
            "Trainable params: 136,241\n",
            "Non-trainable params: 388\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Concatenate, SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D, Dense, Activation, Multiply, Conv2D, BatchNormalization, Add\n",
        "\n",
        "# Input layer\n",
        "input_layer = Input(shape=(128, 128, 1))\n",
        "\n",
        "# First set of separable convolutions\n",
        "conv1_1 = SeparableConv2D(16, (5, 5), padding='same', activation='relu')(input_layer)\n",
        "conv1_2 = SeparableConv2D(16, (1, 1), padding='same', activation='relu')(conv1_1)\n",
        "conv1_3 = SeparableConv2D(16, (3, 3), padding='same', activation='relu')(conv1_2)\n",
        "\n",
        "# Concatenate first set of convolutions\n",
        "concat1 = Concatenate()([conv1_1, conv1_2, conv1_3])\n",
        "\n",
        "# Second set of separable convolutions\n",
        "conv2_1 = SeparableConv2D(32, (3, 3), padding='same', activation='relu')(concat1)\n",
        "conv2_2 = SeparableConv2D(32, (5, 5), padding='same', activation='relu')(concat1)\n",
        "conv2_3 = SeparableConv2D(32, (1, 1), padding='same', activation='relu')(concat1)\n",
        "\n",
        "# Third set of separable convolutions\n",
        "conv3_1 = SeparableConv2D(32, (3,3), padding='same', activation='relu')(conv2_1)\n",
        "conv3_2 = SeparableConv2D(32, (5,5), padding='same', activation='relu')(conv2_2)\n",
        "conv3_3 = SeparableConv2D(32, (1,1), padding='same', activation='relu')(conv2_3)\n",
        "\n",
        "# Concatenate second set of convolutions\n",
        "concat2 = Concatenate()([conv3_1, conv3_2, conv3_3])\n",
        "\n",
        "# Max pooling\n",
        "max_pool = MaxPooling2D(pool_size=(2, 2))(concat2)\n",
        "\n",
        "# Channel Wise Attention (CWA)\n",
        "channels = 96  # Corrected the number of channels\n",
        "\n",
        "# Global Average Pooling\n",
        "global_pooling = GlobalAveragePooling2D()(max_pool)\n",
        "\n",
        "# Fully Connected Layers with ReLU activation\n",
        "fc1 = Dense(channels, activation='relu')(global_pooling)\n",
        "fc2 = Dense(channels, activation='relu')(fc1)\n",
        "\n",
        "# Sigmoid activation for channel-wise importance scores\n",
        "channel_attention = Activation('sigmoid')(fc2)\n",
        "\n",
        "# Expand dimensions for channel multiplication\n",
        "channel_attention = tf.expand_dims(tf.expand_dims(channel_attention, axis=1), axis=1)\n",
        "\n",
        "# Apply channel-wise attention by element-wise multiplication\n",
        "output_feature_map_cwa = Multiply()([max_pool, channel_attention])\n",
        "\n",
        "# Spatial Attention (SA)\n",
        "kernel_size = 9\n",
        "\n",
        "# First branch with 1x9x96 convolution and batch normalization\n",
        "branch1 = Conv2D(96, (1, kernel_size), padding='same')(concat1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Activation('relu')(branch1)\n",
        "\n",
        "# Second branch with 9x1x96 convolution and batch normalization\n",
        "branch2 = Conv2D(96, (kernel_size, 1), padding='same')(concat1)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Activation('relu')(branch2)\n",
        "\n",
        "# Second set of convolutions for spatial aggregation with 9x1x1 and 1x9x1 kernels\n",
        "branch1_2 = Conv2D(1, (kernel_size, 1), padding='same')(branch1)\n",
        "branch1_2 = BatchNormalization()(branch1_2)\n",
        "branch1_2 = Activation('relu')(branch1_2)\n",
        "\n",
        "branch2_2 = Conv2D(1, (1, kernel_size), padding='same')(branch2)\n",
        "branch2_2 = BatchNormalization()(branch2_2)\n",
        "branch2_2 = Activation('relu')(branch2_2)\n",
        "\n",
        "# Combine the two branches by element-wise addition\n",
        "combined = Add()([branch1_2, branch2_2])\n",
        "\n",
        "# Normalize using a sigmoid operation\n",
        "sa_matrix = Activation('sigmoid')(combined)\n",
        "\n",
        "# Apply the SA matrix to the input feature map\n",
        "output_feature_map_sa = Multiply()([input_layer, sa_matrix])\n",
        "\n",
        "# Resize SA feature map to match CWA spatial dimensions\n",
        "output_feature_map_sa_resized = tf.image.resize(output_feature_map_sa, (64, 64), method='bilinear')\n",
        "\n",
        "# Resize CWA feature map to match SA spatial dimensions\n",
        "output_feature_map_cwa_resized = tf.image.resize(output_feature_map_cwa, (64, 64), method='bilinear')\n",
        "\n",
        "# Concatenate SA with CWA\n",
        "concat_sa_cwa = Concatenate()([output_feature_map_sa_resized, output_feature_map_cwa_resized])\n",
        "\n",
        "\n",
        "\n",
        "# Another max pooling\n",
        "max_pool3 = MaxPooling2D(pool_size=(2, 2))(concat_sa_cwa)\n",
        "\n",
        "# Global average pooling\n",
        "global_avg_pool = GlobalAveragePooling2D()(max_pool3)\n",
        "\n",
        "# Dense layers\n",
        "dense1 = Dense(128, activation='relu')(global_avg_pool)\n",
        "dense2 = Dense(64, activation='relu')(dense1)\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(2, activation='softmax')(dense2)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',  # You can choose a different optimizer\n",
        "              loss='sparse_categorical_crossentropy',  # Use 'categorical_crossentropy' for one-hot encoded labels\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90WhdfxWG40d",
        "outputId": "bccbe342-98f2-4bed-bdef-9cbde194dd52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "30/30 [==============================] - 316s 10s/step - loss: 3.0038e-06 - accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "30/30 [==============================] - 326s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "30/30 [==============================] - 321s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "30/30 [==============================] - 328s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "30/30 [==============================] - 324s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "30/30 [==============================] - 332s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "30/30 [==============================] - 329s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "30/30 [==============================] - 343s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "30/30 [==============================] - 331s 11s/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "30/30 [==============================] - 347s 12s/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7e73ec88cc10>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "model.fit(X_train, Y_train, epochs=10, batch_size=32)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
